# Ship-a-Service: End-to-End CI/CD on Managed Kubernetes
Launch EKS Cluster with Terraform
1ï¸âƒ£ Set AWS credentials

Export your AWS keys and region:

export AWS_ACCESS_KEY_ID=<your_access_key>
export AWS_SECRET_ACCESS_KEY=<your_secret_key>
export AWS_DEFAULT_REGION=<your_region>


Check they are valid:

aws sts get-caller-identity

2ï¸âƒ£ Make sure your SSH key exists

If you want SSH access to nodes:

Check existing keys:

aws ec2 describe-key-pairs


Create key if missing:

aws ec2 create-key-pair --key-name ahmedkey --query 'KeyMaterial' --output text > ahmedkey.pem
chmod 400 ahmedkey.pem


Update Terraform variable ssh_key_name = "ahmedkey"

3ï¸âƒ£ Initialize Terraform
terraform init

4ï¸âƒ£ Plan deployment
terraform plan


Check that Terraform plans to create your VPC, subnets, EKS cluster, node groups, and ECR repository.

5ï¸âƒ£ Apply deployment
terraform apply


Type yes when prompted.

Terraform will create all resources, including node group using your SSH key.
**Author:** Ahmed Marzougui (@MarzouguiAhmed9)  
**Challenge:** DevOps/Technical Writer Position  
**Status:** Phase 1 Complete - Infrastructure Provisioning âœ…

---

## ğŸ“‹ Overview

This project demonstrates production-ready infrastructure provisioning for a Kubernetes-based microservices platform on AWS EKS, showcasing infrastructure-as-code best practices and technical documentation skills.

**Goal:** Build complete CI/CD pipeline: `commit â†’ container â†’ security checks â†’ Helm deploy â†’ production`

**Current Status:** Infrastructure foundation complete (Phase 1 of 7)

---

## ğŸ¯ What's Been Built (Phase 1)

### Infrastructure Components

âœ… **Network Layer**
- Custom VPC (10.0.0.0/16)
- 2 Public subnets across different Availability Zones
- Multi-AZ high availability setup

âœ… **Kubernetes Cluster**
- AWS EKS v1.28 managed cluster
- Auto-scaling node group (1-3 nodes)
- t3.medium instances (2 vCPU, 4GB RAM)

âœ… **Container Registry**
- Private ECR repository
- Image scanning enabled
- Mutable tags for development

âœ… **IAM & Security**
- EKS cluster role with minimal permissions
- Node group IAM role
- GitLab CI OIDC integration prepared

---

## ğŸ—ï¸ Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AWS Region: us-east-1                   â”‚
â”‚                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚              VPC (10.0.0.0/16)                       â”‚ â”‚
â”‚  â”‚                                                      â”‚ â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚ â”‚
â”‚  â”‚   â”‚  Subnet A      â”‚      â”‚  Subnet B      â”‚        â”‚ â”‚
â”‚  â”‚   â”‚  10.0.1.0/24   â”‚      â”‚  10.0.2.0/24   â”‚        â”‚ â”‚
â”‚  â”‚   â”‚  us-east-1a    â”‚      â”‚  us-east-1b    â”‚        â”‚ â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚ â”‚
â”‚  â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚ â”‚
â”‚  â”‚                    â”‚                                 â”‚ â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚ â”‚
â”‚  â”‚   â”‚  EKS Cluster: ship-a-service     â”‚               â”‚ â”‚
â”‚  â”‚   â”‚  Version: 1.28                   â”‚               â”‚ â”‚
â”‚  â”‚   â”‚                                  â”‚               â”‚ â”‚
â”‚  â”‚   â”‚  Node Group:                     â”‚               â”‚ â”‚
â”‚  â”‚   â”‚  â€¢ Min: 1 node                   â”‚               â”‚ â”‚
â”‚  â”‚   â”‚  â€¢ Desired: 2 nodes              â”‚               â”‚ â”‚
â”‚  â”‚   â”‚  â€¢ Max: 3 nodes                  â”‚               â”‚ â”‚
â”‚  â”‚   â”‚  â€¢ Type: t3.medium               â”‚               â”‚ â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  ECR Repository: ship-a-service                      â”‚ â”‚
â”‚  â”‚  â€¢ Image Scanning: Enabled                           â”‚ â”‚
â”‚  â”‚  â€¢ Encryption: AES-256                               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  IAM Roles                                           â”‚ â”‚
â”‚  â”‚  â€¢ EKS Cluster Role                                  â”‚ â”‚
â”‚  â”‚  â€¢ Node Group Role                                   â”‚ â”‚
â”‚  â”‚  â€¢ GitLab CI Role (OIDC)                             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Prerequisites

### Required Tools

| Tool | Version | Purpose |
|------|---------|---------|
| Terraform | â‰¥ 1.5, < 2.0 | Infrastructure provisioning |
| AWS CLI | â‰¥ 2.0 | AWS authentication |
| kubectl | â‰¥ 1.28 | Kubernetes management |

### AWS Setup

**1. Create AWS Account**
- Free tier at [aws.amazon.com/free](https://aws.amazon.com/free)

**2. Create IAM User**
Required policies:
- `AmazonEKSClusterPolicy`
- `AmazonEC2FullAccess`
- `AmazonECRFullAccess`
- `IAMFullAccess`

**3. Create EC2 Key Pair**
```bash
aws ec2 create-key-pair \
  --key-name ahmedkey \
  --query 'KeyMaterial' \
  --output text > ~/.ssh/ahmedkey.pem

chmod 400 ~/.ssh/ahmedkey.pem
```

---

## ğŸš€ Quick Start

### 1. Clone Repository
```bash
git clone https://github.com/MarzouguiAhmed9/ship-a-service-end-to-end-ci-cd-on-managed-kubernetes.git
cd ship-a-service-end-to-end-ci-cd-on-managed-kubernetes/infra/terraform
```

### 2. Configure AWS Credentials
```bash
export AWS_ACCESS_KEY_ID="your_access_key"
export AWS_SECRET_ACCESS_KEY="your_secret_key"
export AWS_DEFAULT_REGION="us-east-1"

# Verify
aws sts get-caller-identity
```

### 3. Initialize Terraform
```bash
terraform init
```

### 4. Review Configuration

**Create `terraform.tfvars`:**
```hcl
# Cluster
cluster_name = "ship-a-service"
region       = "us-east-1"

# Network
vpc_cidr      = "10.0.0.0/16"
subnet_a_cidr = "10.0.1.0/24"
subnet_b_cidr = "10.0.2.0/24"
az_a          = "us-east-1a"
az_b          = "us-east-1b"

# Nodes
node_desired  = 2
node_min      = 1
node_max      = 3
node_type     = "t3.medium"

# SSH
ssh_key_name  = "ahmedkey"

# Environment
env = "dev"
```

### 5. Deploy Infrastructure
```bash
# Preview changes
terraform plan

# Apply (15-20 minutes)
terraform apply
# Type: yes
```

### 6. Connect to Cluster
```bash
# Configure kubectl
aws eks update-kubeconfig \
  --region us-east-1 \
  --name ship-a-service

# Verify
kubectl get nodes
```

**Expected Output:**
```
NAME                          STATUS   ROLES    AGE   VERSION
ip-10-0-1-xxx.ec2.internal    Ready    <none>   5m    v1.28.x
ip-10-0-2-xxx.ec2.internal    Ready    <none>   5m    v1.28.x
```

---

## ğŸ“ Terraform Structure

```
infra/terraform/
â”œâ”€â”€ main.tf          # VPC, subnets, EKS cluster, ECR, IAM roles
â”œâ”€â”€ variables.tf     # Input variables
â”œâ”€â”€ outputs.tf       # Cluster endpoint, ECR URL
â”œâ”€â”€ versions.tf      # Provider versions (AWS ~5.0)
â””â”€â”€ terraform.tfvars # Your configuration values (not committed)
```

### Key Resources Created

**Network (3 resources):**
- `aws_vpc.main` - VPC with CIDR 10.0.0.0/16
- `aws_subnet.subnet_a` - Public subnet in AZ-A
- `aws_subnet.subnet_b` - Public subnet in AZ-B

**EKS (via module - ~20 resources):**
- EKS control plane
- Managed node group with auto-scaling
- Security groups
- IAM roles and policies

**Registry (1 resource):**
- `aws_ecr_repository.app` - Private container registry

**IAM (2 resources):**
- `aws_iam_role.gitlab_ci_role` - GitLab CI OIDC role
- Policy attachments for ECR and EKS access

---

## ğŸ“Š Outputs

After deployment, retrieve important values:

```bash
# Get all outputs
terraform output

# ECR repository URL
terraform output ecr_repository_url
# Output: xxxxx.dkr.ecr.us-east-1.amazonaws.com/ship-a-service

# Cluster name
terraform output cluster_name
# Output: ship-a-service

# Cluster endpoint
terraform output cluster_endpoint
# Output: https://xxxxx.eks.amazonaws.com
```

---

## ğŸ” Security Features

### IAM Least Privilege

**EKS Cluster Role:**
```
Managed Policies:
  âœ“ AmazonEKSClusterPolicy (AWS managed)
  âœ“ AmazonEKSVPCResourceController (AWS managed)
```

**Node Group Role:**
```
Managed Policies:
  âœ“ AmazonEKSWorkerNodePolicy
  âœ“ AmazonEKS_CNI_Policy
  âœ“ AmazonEC2ContainerRegistryReadOnly
```

**GitLab CI Role (OIDC):**
```
Assume Role: Web Identity via GitLab OIDC
Permissions:
  âœ“ AmazonEC2ContainerRegistryPowerUser (push images)
  âœ“ AmazonEKSClusterPolicy (deploy to cluster)
  
Condition: project_path:your-group/your-project:*
```

### Encryption
- âœ… ECR images encrypted at rest (AES-256)
- âœ… EKS control plane encrypted by default
- âœ… No hardcoded credentials (uses AWS IAM)

---

## ğŸ’° Cost Estimate

| Resource | Quantity | Cost/Hour | Monthly Cost |
|----------|----------|-----------|--------------|
| EKS Control Plane | 1 | $0.10 | $73.00 |
| t3.medium nodes | 2 | $0.0416 | $59.90 |
| ECR Storage | <1 GB | - | $0.10 |
| **TOTAL** | | | **~$133/month** |

### Cost Optimization

**For Development:**
```hcl
# Scale down to 1 node
node_desired = 1
node_min     = 1
node_max     = 1

# Use smaller instance
node_type = "t3.small"  # $0.0208/hour (~50% savings)

# Estimated savings: ~$30/month
```

**Note:** EKS control plane ($73/month) is NOT free tier eligible.

---

## ğŸ”§ Troubleshooting

### Issue: Key Pair Not Found
```
Error: The key pair 'ahmedkey' does not exist
```

**Fix:**
```bash
aws ec2 create-key-pair --key-name ahmedkey \
  --query 'KeyMaterial' --output text > ~/.ssh/ahmedkey.pem
chmod 400 ~/.ssh/ahmedkey.pem
terraform apply
```

### Issue: Provider Version Error
```
Error: Unsupported block type "elastic_gpu_specifications"
```

**Fix:**
```bash
rm -rf .terraform .terraform.lock.hcl
terraform init -upgrade
```

### Issue: kubectl Connection Failed
```
Error: Unable to connect to the server
```

**Fix:**
```bash
aws eks update-kubeconfig --name ship-a-service --region us-east-1
kubectl get svc
```

### Useful Commands

```bash
# Check Terraform state
terraform state list
terraform show

# AWS verification
aws eks describe-cluster --name ship-a-service
aws ecr describe-repositories

# Kubernetes checks
kubectl get nodes
kubectl get pods --all-namespaces
kubectl cluster-info
```

---

## ğŸ§¹ Cleanup

### Destroy Infrastructure

```bash
cd infra/terraform

# Preview
terraform plan -destroy

# Destroy (10-15 minutes)
terraform destroy
# Type: yes
```

### Manual Cleanup (if needed)

```bash
# Delete ECR images first
aws ecr batch-delete-image \
  --repository-name ship-a-service \
  --image-ids imageTag=latest

# Delete stuck security groups
aws ec2 describe-security-groups \
  --filters "Name=vpc-id,Values=<vpc-id>" \
  --query 'SecurityGroups[*].GroupId' \
  --output text | xargs -I {} aws ec2 delete-security-group --group-id {}
```

---


## ğŸ’° Cost Guardrails
terraform refresh && terraform output cost_report

## ğŸ“š Project Structure

```
ship-a-service-end-to-end-ci-cd-on-managed-kubernetes/
â”‚
â”œâ”€â”€ infra/terraform/          âœ… COMPLETE
â”‚   â”œâ”€â”€ main.tf              # Infrastructure definitions
â”‚   â”œâ”€â”€ variables.tf         # Input variables
â”‚   â”œâ”€â”€ outputs.tf           # Output values
â”‚   â”œâ”€â”€ versions.tf          # Provider versions
â”‚   â””â”€â”€ terraform.tfvars     # Configuration (gitignored)
â”‚
â”œâ”€â”€ app/                      â³ NEXT PHASE
â”‚   â”œâ”€â”€ src/server.py        # HTTP service
â”‚   â”œâ”€â”€ tests/               # Unit tests
â”‚   â”œâ”€â”€ Dockerfile           # Multi-stage build
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ charts/app/               â³ FUTURE
â”‚   â”œâ”€â”€ Chart.yaml
â”‚   â”œâ”€â”€ values.dev.yaml
â”‚   â”œâ”€â”€ values.prod.yaml
â”‚   â””â”€â”€ templates/
â”‚
â”œâ”€â”€ .github/workflows/        â³ FUTURE
â”‚   â”œâ”€â”€ pr-checks.yml
â”‚   â””â”€â”€ deploy.yml
â”‚
â””â”€â”€ README.md                 âœ… This file
```

---

## ğŸ“– Next Steps

### Phase 2: Ansible (Planned)
- Configure CI runner VM
- Install Docker, kubectl, Helm
- Setup OIDC authentication

### Phase 3: Application (Planned)
- Simple HTTP server with `/healthz`
- Multi-stage Dockerfile
- Unit tests

### Phase 4: Helm Chart 


Chart with dev/prod values

HPA, probes, ingress

Rollout strategy

1ï¸âƒ£ Create the namespace

kubectl create ns dev


2ï¸âƒ£ Create the private Docker Hub secret

kubectl create secret docker-registry regcred \
  --docker-username=ahmed20007 \
  --docker-password=SaidaHamdouni2000! \
  --docker-server=https://index.docker.io/v1/ \
  --namespace dev


3ï¸âƒ£ Deploy the application with Helm

helm upgrade --install app-dev ./ \
  -f values.dev.yaml \
  --namespace dev \
  --atomic \
  --wait \
  --timeout 5m


4ï¸âƒ£ Check that the pods are Running

kubectl get pods -n dev


5ï¸âƒ£ Check the service

kubectl get svc -n dev


Note the name and ClusterIP to test internal access.

6ï¸âƒ£ Test the service from a temporary pod

kubectl run testpod -i --tty --rm --image=busybox --restart=Never -n dev -- sh


Then, inside this pod:

wget -q -O- http://app-dev.dev.svc.cluster.local:8080/healthz


You should see:

{"SYS_ENV":"helloworld","status":"ok"}


7ï¸âƒ£ Check the HPA

kubectl get hpa -n dev -w


HPA shows REPLICAS, CPU %, and will scale pods if CPU exceeds 50%.

8ï¸âƒ£ Test HPA under load

kubectl run load-gen -i --tty --rm --image=busybox --restart=Never -- sh
while true; do wget -q -O- http://app-dev.dev.svc.cluster.local:8080/healthz; done


You should see the number of pods increase in HPA if CPU usage rises.

9ï¸âƒ£ Automatic rollback

If the deployment fails probes or does not become ready:

helm upgrade --install app-dev ./ -f values.dev.yaml --namespace dev --atomic


Helm will automatically roll back to the previous stable version.


### Phase 5: CI/CD (Planned)
GitHub Actions CI Pipeline Testing

This guide explains how to test the CI workflow for this repository using a test branch and pull request. The workflow includes: Go lint & tests, Docker build & Trivy scan, Terraform fmt/validate/plan, Helm lint/unit tests, and TFSEC IaC security scan.

1ï¸âƒ£ Create a test branch
git siwtch test-ci



2ï¸âƒ£ Make a small change to trigger CI
# Example: add a comment
echo "// trigger CI test" >> app/src/main.go
git add app/src/main.go
git commit -m "Test CI workflow"
git push origin test-ci

3ï¸âƒ£ Open a Pull Request

Go to GitHub â†’ your repo â†’ â€œPull Requestsâ€ â†’ â€œNew pull requestâ€

Base: main

Compare: test-ci

This automatically triggers the workflow because it runs on pull_request to main.

4ï¸âƒ£ Check workflow run

Go to the Actions tab â†’ click the latest run â†’ view logs for each step:

1ï¸âƒ£ Checkout repository
2ï¸âƒ£ Setup Go
3ï¸âƒ£ Install dependencies
4ï¸âƒ£ Go lint
5ï¸âƒ£ Run unit tests
6ï¸âƒ£ Docker build
7ï¸âƒ£ Trivy vulnerability scan
8ï¸âƒ£ Upload Trivy SARIF results
9ï¸âƒ£ Setup Terraform
ğŸ”Ÿ Terraform Init
1ï¸âƒ£1ï¸âƒ£ Terraform fmt check
1ï¸âƒ£2ï¸âƒ£ Terraform validate
1ï¸âƒ£3ï¸âƒ£ Terraform plan
1ï¸âƒ£4ï¸âƒ£ Helm lint
1ï¸âƒ£5ï¸âƒ£ Install Helm unittest plugin
1ï¸âƒ£6ï¸âƒ£ Helm unit tests
1ï¸âƒ£7ï¸âƒ£ Install tfsec
1ï¸âƒ£8ï¸âƒ£ TFSEC IaC security scan
---

## ğŸ“Š Progress Tracker

| Phase | Component | Status | Progress |
|-------|-----------|--------|----------|
| 1 | Infrastructure (Terraform) | âœ… Complete | 100% |
| 2 | Build Host (Ansible) | â³ Planned | 0% |
| 3 | Application (Docker) | â³ Planned | 0% |
| 4 | Helm Deployment | â³ Planned | 0% |
| 5 | CI/CD Pipeline | â³ Planned | 0% |
| 6 | Security & Observability | â³ Planned | 0% |
| 7 | Documentation | ğŸš§ In Progress | 50% |
| **OVERALL** | | | **15%** |

---

## ğŸ“ Skills Demonstrated

**Infrastructure as Code:**
- âœ… Terraform module usage (EKS v18.29.0)
- âœ… AWS provider configuration (~v5.0)
- âœ… Variable management
- âœ… Output definitions

**AWS Services:**
- âœ… EKS cluster provisioning
- âœ… VPC and subnet design
- âœ… ECR repository management
- âœ… IAM roles and policies
- âœ… OIDC federation setup

**Kubernetes:**
- âœ… Managed node groups
- âœ… kubectl configuration
- âœ… Cluster connectivity

**Documentation:**
- âœ… Clear README structure
- âœ… Code examples with explanations
- âœ… Troubleshooting guides
- âœ… Architecture diagrams

---

## ğŸ¤ Contact

**Ahmed Marzougui**  
GitHub: [@MarzouguiAhmed9](https://github.com/MarzouguiAhmed9)

---

## ğŸ“ Notes

- This is Phase 1 of a 7-phase DevOps challenge project
- Infrastructure is production-sensible but optimized for demonstration
- All costs are estimates based on us-east-1 pricing (October 2024)
- GitLab OIDC configuration requires project-specific updates

---

**Last Updated:** 2024-10-18 13:29:31 UTC  
**Version:** 1.0.0  
**Status:** Phase 1 Complete âœ…