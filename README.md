# Ship-a-Service: End-to-End CI/CD on Managed Kubernetes
Launch EKS Cluster with Terraform
1Ô∏è‚É£ Set AWS credentials
Export your AWS keys and region:
export AWS_ACCESS_KEY_ID=<your_access_key>
export AWS_SECRET_ACCESS_KEY=<your_secret_key>
export AWS_DEFAULT_REGION=<your_region>


Create key if missing:
aws ec2 create-key-pair --key-name ahmedkey --query 'KeyMaterial' --output text > ahmedkey.pem
chmod 400 ahmedkey.pem



3Ô∏è‚É£ Initialize Terraform
terraform init
4Ô∏è‚É£ Plan deployment
terraform plan
Check that Terraform plans to create your VPC, subnets, EKS cluster, node groups, and ECR repository.
5Ô∏è‚É£ Apply deployment
terraform apply

## üéØ What's Been Built (Phase 1)

Infrastructure r√©seau

Cr√©e un VPC avec DNS activ√©.

Ajoute une Internet Gateway pour le trafic sortant.

D√©finit une route publique (0.0.0.0/0).

Cr√©e 2 subnets publics dans deux zones de disponibilit√© pour HA.

Associe les subnets √† la route table.

EKS (Kubernetes)

D√©ploie un cluster EKS version 1.28.

Cr√©e un managed node group avec un type d‚Äôinstance, nombre min/max de n≈ìuds, et SSH key.

Les nodes sont tagu√©s et r√©partis sur les 2 subnets pour haute disponibilit√©.

ECR (Docker registry)

Cr√©e un repository ECR ship-a-service avec scan d‚Äôimage √† la push et AES256 pour le stockage.

Politique de cycle de vie pour garder les 10 derni√®res images.

IAM & GitHub Actions (CI/CD)

Cr√©e un OIDC provider pour GitHub Actions.

Cr√©e un role IAM pour GitHub Actions avec permissions pour :

Pousser/puller les images dans ECR.

D√©ployer sur le cluster EKS (via une policy personnalis√©e).


Connect to Cluster
```bash
# Configure kubectl
aws eks update-kubeconfig \
  --region us-east-1 \
  --name ship-a-service

# Verify
kubectl get nodes
```

**Expected Output:**
```
NAME                          STATUS   ROLES    AGE   VERSION
ip-10-0-1-xxx.ec2.internal    Ready    <none>   5m    v1.28.x
ip-10-0-2-xxx.ec2.internal    Ready    <none>   5m    v1.28.x
```



## üìä Outputs

1Ô∏è‚É£ Infos techniques utiles pour d√©ploiement

registry_url ‚Üí URL du ECR pour push/pull les images Docker.

cluster_name ‚Üí nom du cluster EKS.

cluster_endpoint ‚Üí endpoint API du cluster pour kubectl.

cluster_certificate_authority_data ‚Üí certificat pour s√©curiser l‚Äôacc√®s Kubernetes.

kubeconfig_yaml ‚Üí fichier kubeconfig complet pr√™t √† copier dans ~/.kube/config.

github_actions_role_arn ‚Üí ARN du role IAM GitHub Actions pour CI/CD.

Ces outputs permettent √† ton √©quipe ou √† GitHub Actions d‚Äôinteragir avec le cluster et le registry facilement.

2Ô∏è‚É£ Estimation et suivi des co√ªts

monthly_cost_estimate ‚Üí d√©tail complet par mois : co√ªt control plane, nodes, ECR, logs, transfert de donn√©es, etc.

cost_report ‚Üí version format√©e et lisible (tableau avec conseils d‚Äôoptimisation et budget).

total_monthly_cost, total_daily_cost, total_hourly_cost ‚Üí r√©sum√© rapide.

budget_status ‚Üí indique si tu es dans le budget (OK, WARNING, OVER BUDGET).

cost_comparison ‚Üí comparaison entre diff√©rentes configurations (minimal/dev/prod).

cost_metadata ‚Üí infos sur la m√©thode de calcul, source, date, etc.


## üìñ Next Steps

### Phase 2: Ansible (Planned)

This phase sets up the CI runner on a remote VM using Ansible.

1Ô∏è‚É£ Pr√©requis

Another VM available with IP address X

Main controller VM with Ansible installed

SSH access from controller to remote VM

2Ô∏è‚É£ Configure SSH Access
# Generate SSH key on main controller (if not already done)
ssh-keygen -t rsa -b 4096 -f ~/.ssh/id_rsa -N ""

# Copy SSH public key to remote VM
ssh-copy-id -i ~/.ssh/id_rsa.pub ansible@X

# Test SSH connection
ssh ansible@X


Replace X with the actual IP of your remote VM. You should be able to SSH without password after this step.

3Ô∏è‚É£ Update Ansible Inventory

Edit inventories/inventory.ini:

[ci_runner]
X ansible_user=ansible


Replace X with the IP of your remote VM.

4Ô∏è‚É£ Test Connection
ansible -i inventories/inventory.ini ci_runner -m ping


Expected output:

X | SUCCESS => {
    "changed": false,
    "ping": "pong"
}

5Ô∏è‚É£ Run Setup Playbook
ansible-playbook -i inventories/dev/hosts.ini playbooks/setup_runner.yml --ask-become-pass


--ask-become-pass will prompt for sudo
‚úÖ Quick Summary of the Playbook

System Prerequisites

Installs: curl, unzip, apt-transport-https, ca-certificates, software-properties-common

Docker

Installs Docker if not already installed

Packages: docker-ce, docker-ce-cli, containerd, docker-compose-plugin, docker-buildx-plugin

Kubernetes CLI (kubectl)

Downloads and installs kubectl to manage Kubernetes clusters

Helm

Installs Helm v3 for managing Kubernetes charts

Terraform

Downloads and installs Terraform v1.8.3

AWS CLI v2

Installs the AWS command-line tool

IAM Role via OIDC (GitHub Actions)

Configures the CI runner to assume an IAM role via GitHub OIDC

Exports AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_SESSION_TOKEN for CI jobs

Docker Handler

Restarts Docker if needed

### Phase 3: Application (Planned)
A minimal Go HTTP service exposing a health endpoint (/healthz) and metrics (/metrics). Container-ready with Docker multi-stage build, runs as non-root, includes HEALTHCHECK, and tracks request count.

Features

/healthz ‚Üí Returns JSON with status and SYS_ENV environment variable.

/metrics ‚Üí Prometheus-style counter: my_app_requests_total.

/ ‚Üí Simple hello endpoint; increments request counter.

Docker multi-stage build.

Non-root user.
Unit-test ready for reliability.

Docker Usage
Build Image
docker build -t app:local .

Run Container
docker run -d -p 8080:8080 --name app-test app:local

Test Health Endpoint
docker exec -it app-test wget -qO- http://localhost:8080/healthz


Expected Output:

{
  "status": "ok",
  "SYS_ENV": "development" // or whatever SYS_ENV you set
}

### Phase 4: Helm Chart 


1Ô∏è‚É£ Helm Chart Overview

Location: charts/app/

Features:

Configurable replicas and resources

Liveness and readiness probes

Ingress (or Gateway) + Service

Horizontal Pod Autoscaler (HPA) based on CPU

Optional custom metrics or requests-per-second (RPS)

Safe rollout strategy (RollingUpdate)

Automated rollback on failed health checks

2Ô∏è‚É£ Directory Structure
charts/app/
‚îú‚îÄ‚îÄ Chart.yaml
‚îú‚îÄ‚îÄ values.dev.yaml
‚îú‚îÄ‚îÄ values.prod.yamlAKIAYS2NT2G72WXJLZUX

MaHgtw4li0ulPR2QisoEiCRq/vvAmyleDduZVbaK
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îú‚îÄ‚îÄ deployment.yaml   # Deployment with rolling update
‚îÇ   ‚îú‚îÄ‚îÄ service.yml       # ClusterIP service
‚îÇ   ‚îú‚îÄ‚îÄ ingress.yml       # Ingress rules
‚îÇ   ‚îú‚îÄ‚îÄ hpa.yaml          # Horizontal Pod Autoscaler
‚îÇ   ‚îî‚îÄ‚îÄ _helpers.tpl      # Template helpers

3Ô∏è‚É£ Deployment Features
Rolling Update Strategy

Gradually replaces old pods

Configurable limits:

maxUnavailable (default: 1)

maxSurge (default: 1)

Safe rollout with --atomic ensures automated rollback if health checks fail

Horizontal Pod Autoscaler

Scales pods automatically based on CPU utilization

Configurable min/max replicas

Monitors /metrics endpoint for additional custom metrics if implemented

Probes & Health Checks

/healthz endpoint for liveness and readiness

--atomic: automatically rolls back if deployment fails

--wait: waits until all resources are ready before finishing====|used in ci workflow
### Phase 5: CI/CD 
GitHub Actions Pipelines

This project has three main workflows:

PR Validation (pr-validation.yml)

Triggered on Pull Requests to main or manually via GitHub.

Steps:

Lint & test Go app

Docker build (no push) + Trivy scan

Terraform fmt/validate/plan

Helm lint + chart unit tests

IaC security scan (tfsec)

Can be run manually from the Actions tab.

Build & Push to Dev / Dev Deploy (build-and-push.yml)

Triggered on merge/push to main or manually.

Steps:

Build & push Docker image to ECR (:main + :<short_sha>)

Terraform apply

Deploy to Dev using Helm (values.dev.yaml)

Post-deploy smoke test (/healthz)

Generate deployment report

Can be triggered manually for testing or redeployment.

Production Promotion (Deploy_to_Production.yml)

Manual approval required to run.

Steps:

Deploy to Production using Helm (values.prod.yaml)

Apply rollout strategy (rolling update with probes)

Automated rollback on failure

Upload SBOM and vulnerability scan reports

Publish deployment summary

Always run manually for controlled production deployment.

===|This clearly links workflow files to their purpose and highlights that all workflows can be triggered manually from GitHub Actions.

### Phase 6: CI/CD 
Secrets & IAM

OIDC-based authentication:
CI pipelines authenticate to AWS using OpenID Connect (OIDC). No long-lived credentials stored in GitHub.

Cloud-native secret storage:
Sensitive data like AWS Account ID is stored in AWS SSM Parameter Store.

### Phase 7: Observability

App metrics:

/metrics endpoint exposes a counter of total requests:

my_app_requests_total 42


/healthz endpoint shows app health and environment info:

{
  "status": "ok",
  "SYS_ENV": "dev"
}


Cloud metrics:

Use CloudWatch to monitor CPU, memory, network, and logs from all pods:

cloudwatch-agent ‚Üí cluster & pod metrics

aws-for-fluent-bit ‚Üí pod logs to CloudWatch Logs

Check HPA & app health:

kubectl get hpa
kubectl describe hpa ship-a-service-app-hpa


